{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784a2e01-4cad-447a-9829-3a5177313814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.tokens import Span\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f83f91-9487-4450-abec-d888b0bac61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate precision, recall, and F1 score for spans\n",
    "def calculate_metrics(annotated_spans, predicted_spans):\n",
    "    true_positives = len(set(annotated_spans) & set(predicted_spans))\n",
    "    false_positives = len(set(predicted_spans) - set(annotated_spans))\n",
    "    false_negatives = len(set(annotated_spans) - set(predicted_spans))\n",
    "    \n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives or false_positives else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives or false_negatives else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision or recall else 0\n",
    "    \n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14041b63-fbbd-4090-9831-6633c1e1acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('predictions_for_evaluation.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf50559-68b4-4a1b-8dcd-73eebf70cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afaf8ab-4c2d-4b85-9e68-4e39bd69fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row of the DataFrame\n",
    "df['precision'], df['recall'], df['f1_score'] = zip(*df.apply(lambda row: calculate_metrics(row['Text'], row['SpanCat_Predictions']), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa9402-61f5-41af-b4f1-286e3a3f8850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evaluation results\n",
    "print(df[['precision', 'recall', 'f1_score']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a92412-aafd-4b7e-9939-107c808426ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate precision, recall, and F1-score for CONLL-type evaluation\n",
    "def calculate_conll_metrics(annotated_spans, predicted_spans):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    # Convert annotated_spans and predicted_spans to sets for efficient comparison\n",
    "    annotated_set = set(annotated_spans)\n",
    "    predicted_set = set(predicted_spans)\n",
    "    \n",
    "    # Calculate true positives\n",
    "    true_positives = len(annotated_set.intersection(predicted_set))\n",
    "    \n",
    "    # Calculate false positives\n",
    "    false_positives = len(predicted_set - annotated_set)\n",
    "    \n",
    "    # Calculate false negatives\n",
    "    false_negatives = len(annotated_set - predicted_set)\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives or false_positives else 0\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives or false_negatives else 0\n",
    "    \n",
    "    # Calculate F1-score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision or recall else 0\n",
    "    \n",
    "    return precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a906fa-2723-487f-9bba-cd0beeec73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row of the DataFrame\n",
    "df['precision_conll'], df['recall_conll'], df['f1_score_conll'] = zip(*df.apply(lambda row: calculate_conll_metrics(row['Text'], row['SpanCat_Predictions']), axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1800ae3c-f308-442a-84d1-32a64bc6be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('predictions_for_evaluation_With_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0d1443-14e6-4407-b3af-a8042702c47b",
   "metadata": {},
   "source": [
    "MUC evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75adcd0a-ccd7-4d1b-bcc6-7734b447aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01955b7d-e9fa-4722-8b0a-06e46d4b0d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c715439-c528-4d86-85f4-0ebed47e2ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af3571-67eb-48d1-a948-4373d6a772c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb2396c-62b6-44b8-b5dc-212e6039c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Handle empty or \"NA\" values in Span1\n",
    "\n",
    "df['span1'] = df['span1'].replace(\"NA\", \"\").fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53bd30a-299e-415d-895f-bd7bd8601dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine if two spans are Correct, Incorrect, Partial, Missing, or Spurious\n",
    "\n",
    "def muc_metrics(gold, pred):\n",
    "    if pred == gold:\n",
    "        return \"COR\"\n",
    "    elif not pred and gold:\n",
    "        return \"MIS\"\n",
    "    elif pred and not gold:\n",
    "        return \"SPU\"\n",
    "    elif pred and gold:\n",
    "        if pred in gold or gold in pred:\n",
    "            return \"PAR\"\n",
    "        else:\n",
    "            return \"INC\"\n",
    "    else:\n",
    "        return \"INC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90423d31-a6d6-469c-96fb-04c8a194e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row in the dataframe\n",
    "\n",
    "df['MUC'] = df.apply(lambda row: muc_metrics(row['Text'], row['span1']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0eba30-540d-42d5-a39c-5cd532edbd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count each metric\n",
    "\n",
    "metrics_counts = df['MUC'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a0a92-ac4d-4d07-89bf-f287102f2e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in counts for any missing metrics\n",
    "\n",
    "for metric in [\"COR\", \"INC\", \"PAR\", \"MIS\", \"SPU\"]:\n",
    "    if metric not in metrics_counts:\n",
    "        metrics_counts[metric] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e13b7-5e93-4070-8180-65c02b085630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the counts for each metric\n",
    "\n",
    "metrics_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a5bfc-a30a-417c-860f-64f6077cd14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a pandas DataFrame\n",
    "\n",
    "metrics_df = pd.DataFrame(list(metrics_counts.items()), columns=['Metric', 'Count'])\n",
    " \n",
    "# Save the DataFrame to a CSV file\n",
    "\n",
    "metrics_df.to_csv('muc_metrics_counts_501_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7950a31-fd99-4c01-bd07-6df868bbf803",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35bbc9d-f07b-47a2-9c0e-4dc269dd3da2",
   "metadata": {},
   "source": [
    "Use this approach\n",
    "\n",
    "it uses wilson score interval and bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d6c7a-2d9b-4fbe-ab8c-c2dc0484a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a661ab-3c7d-4f39-83f0-c54ba115c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5310209d-7015-4e06-b020-5730a8d09242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Handle empty or \"NA\" values \n",
    "\n",
    "df['span1'] = df['span1'].replace(\"NA\", \"\").fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355d03b3-891d-418d-9876-1d4c9852b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine if two spans are correct or partial matches\n",
    "def match_type(gold, pred):\n",
    "    if pred == gold:\n",
    "        return \"exact\"\n",
    "    elif pred in gold or gold in pred:\n",
    "        return \"partial\"\n",
    "    return \"none\"\n",
    " \n",
    "# Apply the function to each row in the dataframe\n",
    "df['match'] = df.apply(lambda row: match_type(row['Text'], row['span1']), axis=1)\n",
    " \n",
    "# Function to calculate precision, recall, and F1 score\n",
    "def calculate_metrics(tp, fp, fn):\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    return precision, recall, f1\n",
    " \n",
    "# Function to calculate Wilson score interval\n",
    "def wilson_score_interval(p, n, z=1.96):\n",
    "    denominator = 1 + z**2 / n\n",
    "    center = p + z**2 / (2 * n)\n",
    "    interval_half_width = z * np.sqrt((p * (1 - p) + z**2 / (4 * n)) / n)\n",
    "    lower_bound = (center - interval_half_width) / denominator\n",
    "    upper_bound = (center + interval_half_width) / denominator\n",
    "    return lower_bound, upper_bound\n",
    " \n",
    "# Function to perform bootstrapping\n",
    "def bootstrap_f1(df, n_bootstraps=1000):\n",
    "    lenient_f1s, strict_f1s = [], []\n",
    "    for _ in range(n_bootstraps):\n",
    "        sample_df = df.sample(n=len(df), replace=True)\n",
    "        lenient_tp = len(sample_df[sample_df['match'].isin([\"exact\", \"partial\"])])\n",
    "        strict_tp = len(sample_df[sample_df['match'] == \"exact\"])\n",
    "        fp = len(sample_df[(sample_df['span1'] != \"\") & (sample_df['match'] == \"none\")])\n",
    "        fn = len(sample_df[(sample_df['span1'] == \"\") & (sample_df['Text'] != \"\")])\n",
    " \n",
    "        _, _, lenient_f1 = calculate_metrics(lenient_tp, fp, fn)\n",
    "        _, _, strict_f1 = calculate_metrics(strict_tp, fp, fn)\n",
    " \n",
    "        lenient_f1s.append(lenient_f1)\n",
    "        strict_f1s.append(strict_f1)\n",
    "    lenient_f1_ci = np.percentile(lenient_f1s, [2.5, 97.5])\n",
    "    strict_f1_ci = np.percentile(strict_f1s, [2.5, 97.5])\n",
    "    return lenient_f1_ci, strict_f1_ci\n",
    " \n",
    "# Calculate lenient and strict metrics\n",
    "lenient_tp = len(df[df['match'].isin([\"exact\", \"partial\"])])\n",
    "strict_tp = len(df[df['match'] == \"exact\"])\n",
    "fp = len(df[(df['span1'] != \"\") & (df['match'] == \"none\")])\n",
    "fn = len(df[(df['span1'] == \"\") & (df['Text'] != \"\")])\n",
    " \n",
    "lenient_precision, lenient_recall, lenient_f1 = calculate_metrics(lenient_tp, fp, fn)\n",
    "strict_precision, strict_recall, strict_f1 = calculate_metrics(strict_tp, fp, fn)\n",
    " \n",
    "# Calculate Wilson score intervals for precision and recall\n",
    "lenient_precision_ci = wilson_score_interval(lenient_precision, lenient_tp + fp)\n",
    "lenient_recall_ci = wilson_score_interval(lenient_recall, lenient_tp + fn)\n",
    "strict_precision_ci = wilson_score_interval(strict_precision, strict_tp + fp)\n",
    "strict_recall_ci = wilson_score_interval(strict_recall, strict_tp + fn)\n",
    " \n",
    "# Perform bootstrapping to get confidence intervals for F1 scores\n",
    "lenient_f1_ci, strict_f1_ci = bootstrap_f1(df)\n",
    " \n",
    "# Create a dictionary with the results\n",
    "results = {\n",
    "    \"Metric\": [\"Precision\", \"Recall\", \"F1 Score\"],\n",
    "    \"Lenient\": [lenient_precision, lenient_recall, lenient_f1],\n",
    "    \"Lenient 95% CI Lower\": [lenient_precision_ci[0], lenient_recall_ci[0], lenient_f1_ci[0]],\n",
    "    \"Lenient 95% CI Upper\": [lenient_precision_ci[1], lenient_recall_ci[1], lenient_f1_ci[1]],\n",
    "    \"Strict\": [strict_precision, strict_recall, strict_f1],\n",
    "    \"Strict 95% CI Lower\": [strict_precision_ci[0], strict_recall_ci[0], strict_f1_ci[0]],\n",
    "    \"Strict 95% CI Upper\": [strict_precision_ci[1], strict_recall_ci[1], strict_f1_ci[1]]\n",
    "}\n",
    " \n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d741e456-e21e-49f9-9411-48a018f3a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('evaluation_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7bbeb3-e89b-411e-9eba-33fa8ea3dc1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

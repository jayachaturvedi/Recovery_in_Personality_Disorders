{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0b7e16-4ff0-405e-9b63-71cee4eeec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0462500-b0f2-43e5-9cb8-8bee6eb59c85",
   "metadata": {},
   "outputs": [],
   "source": [
    " #https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb#scrollTo=XUYo7C-bc8oc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a481f6-dd1f-4041-a683-8a4c51b2520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c641a82-5786-45fd-ab4e-9715fdaddc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m ipykernel install --user --name py312jc2 --display-name \"Python (py312jc2)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfe5b9e-db89-40ba-96c2-76c43ee9ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe5987b-6347-450f-8443-33a39dc2cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version) # should be 3.12.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32005d07-5397-416d-aa93-2ddaa129df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from unsloth import FastLanguageModel  # FastVisionModel for LLMs\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388d40d8-2ffc-4008-b57c-af844b3c4e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__) # 2.5.0+cu124\n",
    "print(\"NumPy version:\", np.__version__) # 2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76fdf8e-39e9-41d7-81e4-d8fef6ecef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572c0526-2890-4221-95e6-fc78b21b556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a26790-5fdb-4b90-863a-98bac6691cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",  # Llama-3.1 2x faster\n",
    "    \"unsloth/Mistral-Small-Instruct-2409\",  # Mistral 22b 2x faster!\n",
    "    \"unsloth/Phi-4\",  # Phi-4 2x faster!\n",
    "    \"unsloth/Phi-4-unsloth-bnb-4bit\",  # Phi-4 Unsloth Dynamic 4-bit Quant\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",  # Gemma 2x faster!\n",
    "    \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"  # Qwen 2.5 2x faster!\n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",  # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "]  # More models at https://docs.unsloth.ai/get-started/all-our-models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1389d22d-bbd1-4688-8395-5e7dada60b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Phi-4\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da581a33-fdfe-4536-84b2-01a1214dc4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfa9a04-d0b0-49fa-85c8-0e2a4cfd8d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"phi-4\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            convo, tokenize = False, add_generation_prompt = False\n",
    "        )\n",
    "        for convo in convos\n",
    "    ]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c889af7d-f9b8-45d9-ad53-71fb61c1121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f7f3d2-65e3-4202-8175-055058d9c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[5][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370e1177-364f-488e-a789-4e8c66255c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[5][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9e652d-186e-490a-8de0-cc9fb0594031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"phi-4\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids = inputs, max_new_tokens = 64, use_cache = True, temperature = 1.5, min_p = 0.1\n",
    ")\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866acbaf-36cf-43a6-b0b0-b03bf338ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(\n",
    "    input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, # do_sample=False, #add to disable randomness \n",
    "    use_cache = True, temperature = 1.5, min_p = 0.1, num_beams=1,      # Single beam ensures no alternative paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0adbda8-5aaa-4a2b-a0d6-030f98526edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"phi4_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"phi4_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7283b0-f8b2-409d-9beb-b31619362f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"phi4_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"does this sentence indicate mental health recovery - he was well kempt\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(\n",
    "    input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "    use_cache = True#, temperature = 1.5, min_p = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5972516b-a567-4d0c-bf3c-58fbcbf3f6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM LEO - this works!\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template \n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer, \n",
    "    chat_template = \"phi-4\", \n",
    ") \n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference \n",
    "\n",
    "messages = [     \n",
    "    {   \n",
    "    \"role\": \"system\",   \n",
    "    \"content\": \"You are an expert clinical NLP assistant. Your task is to identify whether a sentence indicates mental health recovery, including implicit signs. Classify recovery into a predefined domain. Think step-by-step before arriving at a decision. Respond only with structured JSON. Do not include any explanations, comments, or additional text.\" \n",
    "    }, \n",
    "    {   \n",
    "    \"role\": \"user\",   \n",
    "    \"content\": '''Determine whether the following sentence indicates mental health recovery. Think step-by-step through the sentence to determine if it shows any signs of recovery. Then respond in this exact JSON format: { 'recovery': 1 or 0, 'type': '<type from the list below>' } \n",
    "        ### Recovery Types: \n",
    "             - 'social': indicates at least one meaningful social relationship (intimate partner, family member, friend) \n",
    "             - 'occupational': evidence of work, volunteering, vocational training, or study \n",
    "             - 'activities of daily living': ability to organise and manage aspects of daily life such as dressing, hygiene, transportation, shopping, finances, paying bills, meal prep, home maintenance, and medication \n",
    "             - 'personal': shows insight into self and relationship to self \n",
    "             - 'non-derived': no reference to a specific domain \n",
    "             \n",
    "        ### Examples: \n",
    "             - Sentence: 'She is doing well in her job.'   \n",
    "             Response: { 'recovery': 1, 'type': 'occupational' } \n",
    "             - Sentence: 'He is not able to pay his debts.'   \n",
    "             Response: { 'recovery': 0, 'type': 'activities of daily living' } \n",
    "        ### Sentence: He had taken his medication prior to my visits and checked his BMs which were 8mmols  \n",
    "        ''' \n",
    "    } ] \n",
    "\n",
    "inputs = tokenizer.apply_chat_template(     \n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation     \n",
    "    return_tensors = \"pt\", \n",
    ").to(\"cuda\") \n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids = inputs, max_new_tokens = 64, use_cache = True, \n",
    "    do_sample=False, num_beams=1) \n",
    "generated_tokens = outputs[0][inputs.shape[1]:] \n",
    "completion = tokenizer.decode(generated_tokens, \n",
    "                              skip_special_tokens=True).strip() \n",
    "\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fe7708-05ce-418f-b600-b566765e4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to make Leo code to run through df - i think this is now working\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer, \n",
    "    chat_template=\"phi-4\"\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "def generate_response(text):\n",
    "    messages = [\n",
    "        {   \"role\": \"system\",   \"content\": \"You are an expert clinical NLP assistant. Your task is to identify whether a sentence indicates mental health recovery, including implicit signs. Classify recovery into a predefined domain. Think step-by-step before arriving at a decision. Respond only with structured JSON. Do not include any explanations, comments, or additional text.\" },\n",
    "        {   \"role\": \"user\",   \"content\": f'''Determine whether the following sentence indicates mental health recovery. Think step-by-step through the sentence to determine if it shows any signs of recovery. Then respond in this exact JSON format: {{ 'recovery': 1 or 0, 'type': '<type from the list below>' }}\n",
    "            ### Recovery Types:\n",
    "                 - 'social': indicates at least one meaningful social relationship (intimate partner, family member, friend)\n",
    "                 - 'occupational': evidence of work, volunteering, vocational training, or study\n",
    "                 - 'activities of daily living': ability to organise and manage aspects of daily life such as dressing, hygiene, transportation, shopping, finances, paying bills, meal prep, home maintenance, and medication\n",
    "                 - 'personal': shows insight into self and relationship to self\n",
    "                 - 'non-derived': no reference to a specific domain\n",
    "\n",
    "            ### Sentence: {text}  \n",
    "        ''' }\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs, max_new_tokens=64, use_cache=True, do_sample=False, num_beams=1\n",
    "    )\n",
    "    \n",
    "    generated_tokens = outputs[0][inputs.shape[1]:]\n",
    "    completion = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "    return completion\n",
    "\n",
    "# Load DataFrame from CSV \n",
    "df = pd.read_csv('all_matched_of_PD_recovery_using_csv_dataV12-test_set_3-4-ALL.csv')\n",
    "#df = df.sample(n=100, random_state=42)\n",
    "\n",
    "# Ensure 'Text' column exists\n",
    "if 'Text' in df.columns:\n",
    "    llm_outputs = []\n",
    "    start_time = time.time()\n",
    "    for text in df['Text']:\n",
    "        llm_outputs.append(generate_response(text))\n",
    "    df['llm-output'] = llm_outputs\n",
    "    end_time = time.time()\n",
    "    print(f\"Processing completed in {end_time - start_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"Error: 'Text' column not found in DataFrame\")\n",
    "\n",
    "# Save updated DataFrame to CSV\n",
    "df.to_csv('output_data_new_test_set_domain_3_4.csv', index=False)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bf6eda-187f-4514-be1e-5a7dfcef6647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588d379-448a-410b-89a5-038d480617d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652e63b9-3abb-4563-8620-86ecdfa2f1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bef01d-d13c-48b6-98ea-b0beb2a1743f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f30db-232c-4345-9b59-74301b73bc76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72727ca7-46e8-4ed0-ba39-4c29eff90914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65b39b5-486d-4aa6-85de-403e8e5512f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e46b298-da3d-48ee-bbea-320fd269b216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ea14a-8bdb-40cf-b4d0-56b51814d671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e26b967-3b44-4f5c-8aee-692962261de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1c3761-e234-4097-bdfe-376f9770fee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4121cc-ee3f-4720-88c1-0a78e4f8d682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c048612b-7403-4610-9f41-3ccf39f900cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021ed1b5-5a8f-4a7c-9b8a-dc2a10f914b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e7e57d-2d5d-49f4-b4af-e81108470b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb76399e-5ddc-4700-9e77-b851fc392dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55e37c3-711e-4239-a364-f6bb6915aaee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042b6332-366f-4bc7-9e1e-51cfcc207d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a1100-c0d7-45e5-9855-03ec03e6a5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24cf916-d8e4-4405-97bb-5bd3e8bbb727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef066e-92b2-40ff-8406-3714b569150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df.sample(n=5, random_state=42) \n",
    "df_small.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba41a73-5515-4711-a6aa-9397d77e45c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e6d86c-e36c-4494-a297-56b1e340163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to read through dataframe and output result - apply Leo's prompt to it and i have modified it\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template \n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer, \n",
    "    chat_template = \"phi-4\", \n",
    ") \n",
    "\n",
    "\n",
    "# Enable native inference (already done in your code)\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Prepare the function to process each row\n",
    "def generate_output_for_row(row):\n",
    "    # Create the messages to pass to the model\n",
    "    ##old prompt - seems to work best\n",
    "    \n",
    "    messages = [     \n",
    "    {   \n",
    "    \"role\": \"system\",   \n",
    "    \"content\": \"You are a medical professional who is an expert at identifying mental health recovery even when it is implied in text rather than being explicit. Your task is to identify whether a sentence indicates mental health recovery, including implicit mentions. Classify recovery into a predefined domain. Think step-by-step before arriving at a decision. Respond only with structured JSON. Do not include any explanations, comments, or additional text.\" \n",
    "    }, \n",
    "    {   \n",
    "    \"role\": \"user\",   \n",
    "    \"content\": '''Determine whether the following sentence indicates mental health recovery. Respond in this exact JSON format: { 'recovery': 1 or 0, 'type': '<type from the list below>' } \n",
    "        ### Recovery Types: \n",
    "             - 'social': indicates at least one meaningful social relationship (intimate partner, family member, friend) \n",
    "             - 'occupational': evidence of work, volunteering, vocational training, or study \n",
    "             - 'activities of daily living': ability to organise and manage aspects of daily life such as dressing, hygiene, transportation, shopping, finances, paying bills, meal prep, home maintenance, and medication \n",
    "             - 'personal': shows insight into self and relationship to self \n",
    "             - 'non-derived': no reference to a specific domain \n",
    "             \n",
    "        ### Examples: \n",
    "    - Sentence: ‘Difficulty finding job - Now feels too de-motivated to look for job’\n",
    "      Response: { ‘recovery’: 0, ‘type’: ‘occupational’ }\n",
    "    - Sentence: ‘He would not want his son to live without him’\n",
    "      Response: { ‘recovery’: 1, ‘type’: ‘social’ }\n",
    "    - Sentence: ‘Insight Good insight into his difficulties but reactive to make personal changes’\n",
    "      Response: { ‘recovery’: 1, ‘type’: ‘personal’ }\n",
    "    - Sentence: ‘During this time he reports having fallen out of love with his wife’\n",
    "      Response: { ‘recovery’: 0, ‘type’: ‘social’ }\n",
    "    - Sentence: ‘Patient appeared well kempt’\n",
    "      Response: { ‘recovery’: 1, ‘type’: ‘activity of daily living’ }\n",
    "        \n",
    "        ### Sentence: {row}\n",
    "         '''\n",
    "    } ] \n",
    "    \n",
    "\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(     \n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation     \n",
    "    return_tensors = \"pt\", \n",
    ").to(\"cuda\") \n",
    "    \n",
    "    # Streamer for the model output\n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "    \n",
    "    # Generate output from the model\n",
    "    generated_output = model.generate(\n",
    "        input_ids=inputs, \n",
    "        streamer=text_streamer, \n",
    "        max_new_tokens=64,\n",
    "        use_cache=True,\n",
    "        #num_beams=1, # add because Single beam ensures no alternative paths\n",
    "        #do_sample=False #add to disable randomness\n",
    "        temperature=0.7, #remove\n",
    "        #min_p=0.1 #remove\n",
    "        top_p=0.9  #keeps high-probability responses while adding flexibility.\n",
    "    )\n",
    "    \n",
    "    # The generated output can be processed and returned as needed (e.g., decoded)\n",
    "    decoded_output = tokenizer.batch_decode(generated_output, skip_special_tokens=True) \n",
    "    \n",
    "    # Return the first (and likely only) generated output\n",
    "    return decoded_output[0]\n",
    "\n",
    "df_small['output'] = df_small['Text'].apply(generate_output_for_row)\n",
    "# Apply the function to each row of the DataFrame and store the results in a new column 'output'\n",
    "#df_small['output'] = df_small.apply(generate_output_for_row, axis=1)\n",
    "#df_small['output'] = df_small.apply(generate_output_for_row, axis=1)\n",
    "\n",
    "# Now, 'df' will have a new column 'output' with the results\n",
    "df_small.head()\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db15e2-919b-4f0b-a656-02fdbae4908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to read through dataframe and output result\n",
    "\n",
    "# Enable native inference (already done in your code)\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Prepare the function to process each row\n",
    "def generate_output_for_row(row):\n",
    "    # Create the messages to pass to the model\n",
    "    ##old prompt - seems to work best\n",
    "    messages = [\n",
    "   {\"role\": \"system\", \"content\": \"You are a medical professional who is an expert at identifying mental health recovery even when it is implied in text rather than being explicit. Precisely give the output, no extra description is needed\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "    Identify whether the following sentence indicates mental health recovery\n",
    "    \n",
    "    Respond with type of recovery and 1 for recovery and 0 for no recovery.\n",
    "    \n",
    "    Here are the types of recovery:\n",
    "    - social: indicates at least one meaningful social relationship (intimate partner, family member, friend)\n",
    "    - occupational: evidence of work, volunteering, vocational training, or study\n",
    "    - activities of daily living: ability to organise and manage aspects of daily life such as dressing, hygiene, transportation, shopping, finances, paying bills, meal prep, home maintenance, and medication\n",
    "    - personal: shows insight into self and relationship to self\n",
    "    - non-derived: no reference to a specific domain\n",
    "    \n",
    "    Here are some examples:\n",
    "    - sentence: she is doing well in her job.\n",
    "      response: recovery=1, type=occupational\n",
    "    \n",
    "    - sentence: he is not able to pay his debts.\n",
    "      response: recovery=0, type=activities of daily living\n",
    "    \"\"\"}\n",
    "]\n",
    "    # Tokenize and process the messages\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,  # Must add for generation\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Streamer for the model output\n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "    \n",
    "    # Generate output from the model\n",
    "    generated_output = model.generate(\n",
    "        input_ids=inputs, \n",
    "        streamer=text_streamer, \n",
    "        max_new_tokens=128,\n",
    "        use_cache=True,\n",
    "        #num_beams=1, # add because Single beam ensures no alternative paths\n",
    "        #do_sample=False #add to disable randomness\n",
    "        temperature=1.5, #remove\n",
    "        min_p=0.1 #remove\n",
    "    )\n",
    "    \n",
    "    # The generated output can be processed and returned as needed (e.g., decoded)\n",
    "    decoded_output = tokenizer.batch_decode(generated_output, skip_special_tokens=True)\n",
    "    \n",
    "    # Return the first (and likely only) generated output\n",
    "    return decoded_output[0]\n",
    "\n",
    "# Apply the function to each row of the DataFrame and store the results in a new column 'output'\n",
    "#df_small['output'] = df_small.apply(generate_output_for_row, axis=1)\n",
    "df_small['output'] = df_small['Text'].apply(generate_output_for_row, axis=1)\n",
    "\n",
    "# Now, 'df' will have a new column 'output' with the results\n",
    "df_small.head()\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3561b39-63a2-4f53-bd93-ac6126bc0810",
   "metadata": {},
   "source": [
    "start time: 11:58 - for all 13 k - it had been 6 hours and it was still going so i stopped it.\n",
    "\n",
    "df_small - random 100 from data test set domains 3 and 4\n",
    "start time: 12:26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92247d01-02f1-47eb-aee5-912d93117b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small.to_csv('outputs_small_old_prompt_3-4-test.csv')\n",
    "#df.to_csv('outputs_domain3-4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416ce76d-6e4a-4402-b98a-7eafc62b56c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file\n",
    "#df = pd.read_csv(\"outputs_small.csv\")\n",
    "df = pd.read_csv(\"outputs_small_old_prompt_3-4.csv\")\n",
    "\n",
    "# Define a function to remove text before \"assistant\"\n",
    "def remove_text_before_assistant(text):\n",
    "    if pd.isna(text):  # Check for NaN values\n",
    "        return text\n",
    "    keyword = \"assistant\"\n",
    "    index = text.lower().find(keyword)  # Find the position of \"assistant\" (case-insensitive)\n",
    "    return text[index:] if index != -1 else text  # Keep text from \"assistant\" onwards\n",
    "\n",
    "# Apply the function to the column (replace 'your_column' with the actual column name)\n",
    "df[\"output_clean\"] = df[\"output\"].apply(remove_text_before_assistant)\n",
    "\n",
    "# Save the modified Excel file\n",
    "df.to_csv(\"cleaned_output_small_old_prompt_3-4.csv\", index=False)\n",
    "\n",
    "print(\"Text before 'assistant' removed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8698890-12fb-486d-bd05-12a1338bed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get current time\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Define filename\n",
    "filename = \"end_time.txt\"\n",
    "\n",
    "# Save to a text file\n",
    "with open(filename, \"w\") as file:\n",
    "    file.write(\"End time: \" + current_time)\n",
    "\n",
    "print(f\"Time saved to {filename} successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e6670e-c3a5-4a48-ab60-d9a3cd8beb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleaned_output.csv\")\n",
    "df.to_csv('cleaned_output_domain3-4_default_settings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f6349-4326-408d-81f0-6e157010924f",
   "metadata": {},
   "source": [
    "**calculate some metrics after checking and cleaning the file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e2abdd-f2bc-4da9-aa44-632b22b16e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from scipy.stats import bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9cab50-c378-47ee-b134-c92632996d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file\n",
    "df = pd.read_csv(\"cleaned_output.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38af7f5-9f31-47ff-8756-e759a8f14683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there are no missing values\n",
    "df = df.dropna(subset=[\"code\", \"code-from-llm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e4bfc6-e8ef-4a06-803a-d7f158517fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true labels and predictions\n",
    "true_labels = df[\"code\"].values\n",
    "pred_labels = df[\"code-from-llm\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acce02b-5947-4650-bbc2-959827ec3139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision, recall, and F1-score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a25a702-b890-4f87-ab98-9b740fd56b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print scores\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5d104b-8ad7-4ac3-8a56-eecd0bad659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute metric for bootstrapping\n",
    "def metric_fn(indices):\n",
    "    sample_preds = pred_labels[indices]\n",
    "    sample_labels = true_labels[indices]\n",
    "    p, r, f, _ = precision_recall_fscore_support(sample_labels, sample_preds, average=\"weighted\")\n",
    "    return np.array([p, r, f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc40abe-c4ff-48b8-aeef-2514b3c0683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform bootstrap resampling for confidence intervals\n",
    "boot_results = bootstrap(\n",
    "    data=(np.arange(len(true_labels)),),  # Sample indices\n",
    "    statistic=metric_fn,\n",
    "    confidence_level=0.95,\n",
    "    random_state=42,\n",
    "    method='percentile'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8d6302-6d17-454c-80c8-7bdc4b4fd23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract confidence intervals\n",
    "precision_ci = boot_results.confidence_interval[0]\n",
    "recall_ci = boot_results.confidence_interval[1]\n",
    "f1_ci = boot_results.confidence_interval[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b85cd0-156a-4c4b-a5ef-4d0f6bc7bbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confidence intervals\n",
    "print(f\"Precision: {precision:.4f} (95% CI: {precision_ci[0]:.4f} - {precision_ci[1]:.4f})\")\n",
    "print(f\"Recall: {recall:.4f} (95% CI: {recall_ci[0]:.4f} - {recall_ci[1]:.4f})\")\n",
    "print(f\"F1 Score: {f1:.4f} (95% CI: {f1_ci[0]:.4f} - {f1_ci[1]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1260dba3-29e1-44a1-ac4e-e597b60dba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Precision\", \"Recall\", \"F1 Score\"],\n",
    "    \"Value\": [precision, recall, f1],\n",
    "    \"95% CI Lower\": [precision_ci[0], recall_ci[0], f1_ci[0]],\n",
    "    \"95% CI Upper\": [precision_ci[1], recall_ci[1], f1_ci[1]]\n",
    "})\n",
    "\n",
    "# Save metrics to CSV\n",
    "metrics_df.to_csv(\"metrics.csv\", index=False)\n",
    "\n",
    "print(\"Metrics saved successfully to metrics.csv!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f4f2a-a51e-461d-b55e-33cbcd0982c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "885d9d33-029e-4b65-aa49-5f981f03fc33",
   "metadata": {},
   "source": [
    "**For reasoning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f730de-2f65-4990-8069-9a288c273ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4202b90-a493-4a86-b4ac-f7276148139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import torch\n",
    "max_seq_length = 512 # Can increase for longer reasoning traces\n",
    "lora_rank = 16 # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Phi-4\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.7, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = lora_rank,\n",
    "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a60a6a-c8f8-4e2f-a3ed-e99c98c5d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load and prep dataset\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n",
    "    data = data.map(lambda x: { # type: ignore\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    }) # type: ignore\n",
    "    return data # type: ignore\n",
    "\n",
    "dataset = get_gsm8k_questions()\n",
    "\n",
    "# Reward functions\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def count_xml(text) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
    "    return count\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a2f24e-86e2-424a-846d-8e33b390e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : \"Which is bigger? 9.11 or 9.9?\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a2c12c-4af3-434d-9129-2de0918f6415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model=\"microsoft/Phi-4-mini-instruct\", trust_remote_code=True)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "  max_tokens=500,\n",
    "  temperature=0.0,\n",
    ")\n",
    "\n",
    "output = llm.chat(messages=messages, sampling_params=sampling_params)\n",
    "print(output[0].outputs[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ae4fc-9b2e-437b-afbc-808a03b4a1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7043ad-4181-4849-a7b0-87cee4696f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89235a0f-b3d9-4e34-a58d-3850b12ab434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899e4ac-28d8-4010-b7d4-f2226d5feed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c856d8d-353d-4137-a14a-d4f3484a837b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0fa780-f74e-4679-900d-ae92acb65c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b3adda-c62f-498b-ac89-d4e6e4029843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70003762-0f58-4460-bd6f-cedc4e5f4d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot examples to guide the model\n",
    "\n",
    "prompt = \"\"\"\n",
    "Extract meaningful information related to mental health recovery from the following text. Focus on key aspects like occupation, social recovery, activities of daily living, and personal growth.\n",
    "Social – this indicates at least one meaningful social relationship (intimate partner, family member, friend)\n",
    "Occupational – evidence of work, volunteering, vocational training, or study. This is inclusive of hobbies and caring commitments.\n",
    "Activities of Daily Living – ability to organize and manage aspects of daily life such as dressing; hygiene; transportation; shopping; finances (bills, manage assets); meal prep, home maintenance, communication with others (phone, email); and medications.\n",
    "Personal – shows insight into self, and relationship to self.\n",
    "Non- derived – recovery is indicated in the notes without reference to a specific domain.\n",
    "\n",
    "Example 1:\n",
    "Text: \"he has begun going to classes every day.\"\n",
    "Output: \"He made progress by attening his classes and this shows recovery in acitvity of daily living.\"\n",
    "\n",
    "Example 2:\n",
    "Text: \"she started exercising regularly and engaging with supportive friends.\"\n",
    "Output: \"she showed improvement in social recovery\"\n",
    "\n",
    "Example 3:\n",
    "Text: \"he was well dressed\"\n",
    "Output: \"this shows activity of daily living recovery\"\n",
    "\n",
    "Example 4:\n",
    "Text: \"he was unkempt\"\n",
    "Output: \"this shows deterioration in activity of daily living recovery\"\n",
    "\n",
    "Example 5:\n",
    "Text: \"he broke up with his girlfriend\"\n",
    "Output: \"this shows deterioration in personal recovery\"\n",
    "\n",
    "Now, for the text you want to analyze:\n",
    "Text: \"{your text here}\"\n",
    "Output:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1cc2ea-82ac-4510-995e-3ed80088e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the model with the prompt\n",
    "response = model(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db69156-8abe-4ff2-abc3-f20c125d08cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the extracted information about recovery\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py312jc2)",
   "language": "python",
   "name": "py312jc2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

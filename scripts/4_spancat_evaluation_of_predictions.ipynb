{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784a2e01-4cad-447a-9829-3a5177313814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.tokens import Span\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc962e-8b13-4a87-be01-a411ae785ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"m6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14041b63-fbbd-4090-9831-6633c1e1acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test_data_'+model+'_clean.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d37449-934a-4a9e-901c-ce3539f3e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf50559-68b4-4a1b-8dcd-73eebf70cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1665983f-6779-41b6-b9c0-d68fabbf03d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TextContent'] = df['TextContent'].astype(str)\n",
    "df['Text'] = df['Text'].astype(str)\n",
    "df['span1'] = df['span1'].astype(str)\n",
    "df['span2'] = df['span2'].astype(str)\n",
    "\n",
    "df['label'] = df['label'].astype(str)\n",
    "df['label1'] = df['label1'].astype(str)\n",
    "df['label2'] = df['label2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb2396c-62b6-44b8-b5dc-212e6039c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Handle empty or \"NA\" values in Span1\n",
    "\n",
    "df['span1'] = df['span1'].replace(\"NA\", \"\").fillna(\"\")\n",
    "df['span2'] = df['span2'].replace(\"NA\", \"\").fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35bbc9d-f07b-47a2-9c0e-4dc269dd3da2",
   "metadata": {},
   "source": [
    "# USE THIS approach\n",
    "\n",
    "it uses wilson score interval and bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d6c7a-2d9b-4fbe-ab8c-c2dc0484a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7876564e-94cd-4aa4-92df-1df515e14da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('m6-check-lenient-strict-match-clean.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e128bb8-bd40-492d-946c-b1e0a4f49591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate precision, recall, and F1 score\n",
    "def calculate_metrics(tp, fp, fn):\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    return precision, recall, f1\n",
    " \n",
    "# Function to calculate Wilson score interval\n",
    "def wilson_score_interval(p, n, z=1.96):\n",
    "    denominator = 1 + z**2 / n\n",
    "    center = p + z**2 / (2 * n)\n",
    "    interval_half_width = z * np.sqrt((p * (1 - p) + z**2 / (4 * n)) / n)\n",
    "    lower_bound = (center - interval_half_width) / denominator\n",
    "    upper_bound = (center + interval_half_width) / denominator\n",
    "    return lower_bound, upper_bound\n",
    " \n",
    "# Function to perform bootstrapping\n",
    "def bootstrap_f1(df, n_bootstraps=1000):\n",
    "    lenient_f1s, strict_f1s = [], []\n",
    "    for _ in range(n_bootstraps):\n",
    "        sample_df = df.sample(n=len(df), replace=True)\n",
    "        lenient_tp = len(sample_df[sample_df['match'].isin([\"exact\", \"partial\"])])\n",
    "        strict_tp = len(sample_df[sample_df['match'] == \"exact\"])\n",
    "        fp = len(sample_df[(sample_df['span1'] != \"\") & (sample_df['match'] == \"none\")])\n",
    "        fn = len(sample_df[(sample_df['span1'] == \"\") & (sample_df['Text'] != \"\")])\n",
    " \n",
    "        _, _, lenient_f1 = calculate_metrics(lenient_tp, fp, fn)\n",
    "        _, _, strict_f1 = calculate_metrics(strict_tp, fp, fn)\n",
    " \n",
    "        lenient_f1s.append(lenient_f1)\n",
    "        strict_f1s.append(strict_f1)\n",
    "    lenient_f1_ci = np.percentile(lenient_f1s, [2.5, 97.5])\n",
    "    strict_f1_ci = np.percentile(strict_f1s, [2.5, 97.5])\n",
    "    return lenient_f1_ci, strict_f1_ci\n",
    " \n",
    "# Calculate lenient and strict metrics\n",
    "lenient_tp = len(df[df['match'].isin([\"exact\", \"partial\"])])\n",
    "strict_tp = len(df[df['match'] == \"exact\"])\n",
    "fp = len(df[(df['span1'] != \"\") & (df['match'] == \"no match\")])\n",
    "fn = len(df[(df['span1'] == \"\") & (df['Text'] != \"\")])\n",
    " \n",
    "lenient_precision, lenient_recall, lenient_f1 = calculate_metrics(lenient_tp, fp, fn)\n",
    "strict_precision, strict_recall, strict_f1 = calculate_metrics(strict_tp, fp, fn)\n",
    " \n",
    "# Calculate Wilson score intervals for precision and recall\n",
    "lenient_precision_ci = wilson_score_interval(lenient_precision, lenient_tp + fp)\n",
    "lenient_recall_ci = wilson_score_interval(lenient_recall, lenient_tp + fn)\n",
    "strict_precision_ci = wilson_score_interval(strict_precision, strict_tp + fp)\n",
    "strict_recall_ci = wilson_score_interval(strict_recall, strict_tp + fn)\n",
    " \n",
    "# Perform bootstrapping to get confidence intervals for F1 scores\n",
    "lenient_f1_ci, strict_f1_ci = bootstrap_f1(df)\n",
    " \n",
    "# Create a dictionary with the results\n",
    "results = {\n",
    "    \"Metric\": [\"Precision\", \"Recall\", \"F1 Score\"],\n",
    "    \"Lenient\": [lenient_precision, lenient_recall, lenient_f1],\n",
    "    \"Lenient 95% CI Lower\": [lenient_precision_ci[0], lenient_recall_ci[0], lenient_f1_ci[0]],\n",
    "    \"Lenient 95% CI Upper\": [lenient_precision_ci[1], lenient_recall_ci[1], lenient_f1_ci[1]],\n",
    "    \"Strict\": [strict_precision, strict_recall, strict_f1],\n",
    "    \"Strict 95% CI Lower\": [strict_precision_ci[0], strict_recall_ci[0], strict_f1_ci[0]],\n",
    "    \"Strict 95% CI Upper\": [strict_precision_ci[1], strict_recall_ci[1], strict_f1_ci[1]]\n",
    "}\n",
    " \n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355d03b3-891d-418d-9876-1d4c9852b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine if two spans are correct or partial matches\n",
    "'''def match_type(gold, pred):\n",
    "    if pred == gold:\n",
    "        return \"exact\"\n",
    "    elif pred in gold or gold in pred:\n",
    "        return \"partial\"\n",
    "    return \"none\"'''\n",
    "\n",
    "def match_type(gold, pred):\n",
    "    if pd.isna(pred) or pred.strip() == '' or pred==\"nan\":\n",
    "        return \"no pred (FN)\"\n",
    "    elif pred in gold:\n",
    "        if pred == gold:\n",
    "            return \"complete\"\n",
    "        else:\n",
    "            return \"partial\"\n",
    "    else:\n",
    "        return \"no match\"\n",
    "    \n",
    "\n",
    " \n",
    "# Apply the function to each row in the dataframe\n",
    "df['match'] = df.apply(lambda row: match_type(row['Text'], row['span1']), axis=1)\n",
    "\n",
    "'''\n",
    "# Function to calculate precision, recall, and F1 score\n",
    "def calculate_metrics(tp, fp, fn):\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    return precision, recall, f1'''\n",
    " \n",
    "# Function to calculate Wilson score interval\n",
    "'''def wilson_score_interval(p, n, z=1.96):\n",
    "    denominator = 1 + z**2 / n\n",
    "    center = p + z**2 / (2 * n)\n",
    "    interval_half_width = z * np.sqrt((p * (1 - p) + z**2 / (4 * n)) / n)\n",
    "    lower_bound = (center - interval_half_width) / denominator\n",
    "    upper_bound = (center + interval_half_width) / denominator\n",
    "    return lower_bound, upper_bound'''\n",
    " \n",
    "# Function to perform bootstrapping\n",
    "'''def bootstrap_f1(df, n_bootstraps=1000):\n",
    "    lenient_f1s, strict_f1s = [], []\n",
    "    for _ in range(n_bootstraps):\n",
    "        sample_df = df.sample(n=len(df), replace=True)\n",
    "        lenient_tp = len(sample_df[sample_df['match'].isin([\"exact\", \"partial\"])])\n",
    "        strict_tp = len(sample_df[sample_df['match'] == \"exact\"])\n",
    "        fp = len(sample_df[(sample_df['span1'] != \"\") & (sample_df['match'] == \"none\")])\n",
    "        fn = len(sample_df[(sample_df['span1'] == \"\") & (sample_df['Text'] != \"\")])\n",
    " \n",
    "        _, _, lenient_f1 = calculate_metrics(lenient_tp, fp, fn)\n",
    "        _, _, strict_f1 = calculate_metrics(strict_tp, fp, fn)\n",
    " \n",
    "        lenient_f1s.append(lenient_f1)\n",
    "        strict_f1s.append(strict_f1)\n",
    "    lenient_f1_ci = np.percentile(lenient_f1s, [2.5, 97.5])\n",
    "    strict_f1_ci = np.percentile(strict_f1s, [2.5, 97.5])\n",
    "    return lenient_f1_ci, strict_f1_ci'''\n",
    "''' \n",
    "# Calculate lenient and strict metrics\n",
    "lenient_tp = len(df[df['match'].isin([\"exact\", \"partial\"])])\n",
    "strict_tp = len(df[df['match'] == \"exact\"])\n",
    "fp = len(df[(df['span1'] != \"\") & (df['match'] == \"no match\")])\n",
    "fn = len(df[(df['span1'] == \"\") & (df['Text'] != \"\")])\n",
    " \n",
    "lenient_precision, lenient_recall, lenient_f1 = calculate_metrics(lenient_tp, fp, fn)\n",
    "strict_precision, strict_recall, strict_f1 = calculate_metrics(strict_tp, fp, fn)\n",
    " \n",
    "# Calculate Wilson score intervals for precision and recall\n",
    "#lenient_precision_ci = wilson_score_interval(lenient_precision, lenient_tp + fp)\n",
    "#lenient_recall_ci = wilson_score_interval(lenient_recall, lenient_tp + fn)\n",
    "#strict_precision_ci = wilson_score_interval(strict_precision, strict_tp + fp)\n",
    "#strict_recall_ci = wilson_score_interval(strict_recall, strict_tp + fn)\n",
    " \n",
    "# Perform bootstrapping to get confidence intervals for F1 scores\n",
    "#lenient_f1_ci, strict_f1_ci = bootstrap_f1(df)\n",
    " \n",
    "# Create a dictionary with the results\n",
    "results = {\n",
    "    \"Metric\": [\"Precision\", \"Recall\", \"F1 Score\"],\n",
    "    \"Lenient\": [lenient_precision, lenient_recall, lenient_f1],\n",
    "   # \"Lenient 95% CI Lower\": [lenient_precision_ci[0], lenient_recall_ci[0], lenient_f1_ci[0]],\n",
    "   # \"Lenient 95% CI Upper\": [lenient_precision_ci[1], lenient_recall_ci[1], lenient_f1_ci[1]],\n",
    "    \"Strict\": [strict_precision, strict_recall, strict_f1],\n",
    "   # \"Strict 95% CI Lower\": [strict_precision_ci[0], strict_recall_ci[0], strict_f1_ci[0]],\n",
    "   # \"Strict 95% CI Upper\": [strict_precision_ci[1], strict_recall_ci[1], strict_f1_ci[1]]\n",
    "}\n",
    " \n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef66704-91c6-4ee5-ac0f-a694988c724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a556b51-6cfd-4cef-be30-d0e750e1feb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenient_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec72a9-e4b7-414e-91f0-34a8025a4f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bef5d3-fe44-481a-935e-e307b4df9e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('m6-check-lenient-strict-match2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34950f58-42d9-4f11-a54c-84f9a6355d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('evaluation_metrics_'+model+'_CI_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0cf429-0c00-4389-a2a0-415a77c95309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for two spans\n",
    "\n",
    "def match_type(gold, pred1, pred2):\n",
    "    # Combine predictions (span1 and span2)\n",
    "    combined_pred = set(pred1).union(set(pred2)) if pred1 or pred2 else set()\n",
    "    if combined_pred == set(gold):\n",
    "        return \"exact\"\n",
    "    elif set(gold).issubset(combined_pred) or combined_pred.issubset(set(gold)):\n",
    "        return \"partial\"\n",
    "    return \"none\"\n",
    " \n",
    "# Apply the function to each row in the DataFrame\n",
    "df['match'] = df.apply(lambda row: match_type(row['Text'], row['span1'], row['span2']), axis=1)\n",
    "\n",
    "def bootstrap_f1(df, n_bootstraps=1000):\n",
    "    lenient_f1s, strict_f1s = [], []\n",
    " \n",
    "    for _ in range(n_bootstraps):\n",
    "        sample_df = df.sample(n=len(df), replace=True)\n",
    "        # Calculate lenient and strict true positives\n",
    "        lenient_tp = len(sample_df[sample_df['match'].isin([\"exact\", \"partial\"])])\n",
    "        strict_tp = len(sample_df[sample_df['match'] == \"exact\"])\n",
    " \n",
    "        # False positives: predictions (span1 or span2) that are not true positives\n",
    "        fp = len(sample_df[(sample_df['span1'] != \"\") | (sample_df['span2'] != \"\") & (sample_df['match'] == \"none\")])\n",
    " \n",
    "        # False negatives: gold standards (TextContent) missed by both span1 and span2\n",
    "        fn = len(sample_df[(sample_df['span1'] == \"\") & (sample_df['span2'] == \"\") & (sample_df['Text'] != \"\")])\n",
    " \n",
    "        # Calculate lenient and strict F1 scores\n",
    "        _, _, lenient_f1 = calculate_metrics(lenient_tp, fp, fn)\n",
    "        _, _, strict_f1 = calculate_metrics(strict_tp, fp, fn)\n",
    " \n",
    "        lenient_f1s.append(lenient_f1)\n",
    "        strict_f1s.append(strict_f1)\n",
    " \n",
    "    # Calculate confidence intervals for F1 scores\n",
    "    lenient_f1_ci = np.percentile(lenient_f1s, [2.5, 97.5])\n",
    "    strict_f1_ci = np.percentile(strict_f1s, [2.5, 97.5])\n",
    " \n",
    "    return lenient_f1_ci, strict_f1_ci\n",
    "\n",
    "# Calculate lenient and strict true positives\n",
    "lenient_tp = len(df[df['match'].isin([\"exact\", \"partial\"])])\n",
    "strict_tp = len(df[df['match'] == \"exact\"])\n",
    " \n",
    "# False positives: predictions (span1 or span2) that are not true positives\n",
    "fp = len(df[(df['span1'] != \"\") | (df['span2'] != \"\") & (df['match'] == \"none\")])\n",
    " \n",
    "# False negatives: gold standards (TextContent) missed by both span1 and span2\n",
    "fn = len(df[(df['span1'] == \"\") & (df['span2'] == \"\") & (df['Text'] != \"\")])\n",
    " \n",
    "# Calculate lenient and strict metrics\n",
    "lenient_precision, lenient_recall, lenient_f1 = calculate_metrics(lenient_tp, fp, fn)\n",
    "strict_precision, strict_recall, strict_f1 = calculate_metrics(strict_tp, fp, fn)\n",
    "\n",
    "# Calculate Wilson score intervals for precision and recall\n",
    "lenient_precision_ci = wilson_score_interval(lenient_precision, lenient_tp + fp)\n",
    "lenient_recall_ci = wilson_score_interval(lenient_recall, lenient_tp + fn)\n",
    "strict_precision_ci = wilson_score_interval(strict_precision, strict_tp + fp)\n",
    "strict_recall_ci = wilson_score_interval(strict_recall, strict_tp + fn)\n",
    "\n",
    "# Perform bootstrapping to get confidence intervals for F1 scores\n",
    "lenient_f1_ci, strict_f1_ci = bootstrap_f1(df)\n",
    " \n",
    "# Create a dictionary with the results\n",
    "results = {\n",
    "    \"Metric\": [\"Precision\", \"Recall\", \"F1 Score\"],\n",
    "    \"Lenient\": [lenient_precision, lenient_recall, lenient_f1],\n",
    "    \"Lenient 95% CI Lower\": [lenient_precision_ci[0], lenient_recall_ci[0], lenient_f1_ci[0]],\n",
    "    \"Lenient 95% CI Upper\": [lenient_precision_ci[1], lenient_recall_ci[1], lenient_f1_ci[1]],\n",
    "    \"Strict\": [strict_precision, strict_recall, strict_f1],\n",
    "    \"Strict 95% CI Lower\": [strict_precision_ci[0], strict_recall_ci[0], strict_f1_ci[0]],\n",
    "    \"Strict 95% CI Upper\": [strict_precision_ci[1], strict_recall_ci[1], strict_f1_ci[1]]\n",
    "}\n",
    " \n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d741e456-e21e-49f9-9411-48a018f3a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('evaluation_metrics_'+model+'_CI_v2_2spans.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c94321a-5da4-4bfc-a738-129548a40740",
   "metadata": {},
   "source": [
    "for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea809b-bb1e-4458-9c09-964dc9047c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66fcaf1-d7aa-4b99-919d-791ddfc9486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df['label']\n",
    "y_pred = df['label1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6477e19-d1f6-45f9-98be-d4c607e81266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine label1 and label2 using an intersection-like approach (logical AND)\n",
    "y_true = df['label']\n",
    "y_pred = df['label1']\n",
    "\n",
    " \n",
    "# Now you can calculate metrics such as precision, recall, and F1 score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    " \n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    " \n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec38540b-2064-423a-b8be-eaa4e9075fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine label1 and label2 using an intersection-like approach (logical AND)\n",
    "y_true = df['label']\n",
    "y_pred = df.apply(lambda row: set([row['label1']]).intersection([row['label2']]), axis=1)\n",
    " \n",
    "# Convert to lists to match the original format, or keep them as sets if working with multilabel\n",
    "y_pred_combined = y_pred.apply(lambda x: list(x))\n",
    " \n",
    "# Now you can calculate metrics such as precision, recall, and F1 score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    " \n",
    "precision = precision_score(y_true, y_pred_combined, average='macro')\n",
    "recall = recall_score(y_true, y_pred_combined, average='macro')\n",
    "f1 = f1_score(y_true, y_pred_combined, average='macro')\n",
    " \n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9258b50-d8f8-4a52-a2fa-b3bf2a2296bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    " \n",
    "# Combine label1 and label2 as multi-label predictions (e.g., union of the two)\n",
    "y_true = df['label']\n",
    "y_pred = df.apply(lambda row: [row['label1'], row['label2']], axis=1)\n",
    " \n",
    "# Convert to binary indicator matrix (one-hot encoding for multi-label classification)\n",
    "mlb = MultiLabelBinarizer()\n",
    " \n",
    "# Fit and transform both y_true and y_pred to binary format\n",
    "# Make sure y_true is also in a list of lists (multi-label format)\n",
    "y_true_binarized = mlb.fit_transform([[label] for label in y_true])  # Convert y_true to list of lists\n",
    "y_pred_binarized = mlb.transform(y_pred)  # Convert y_pred (label1 + label2 combined) to binary format\n",
    " \n",
    "# Calculate precision, recall, and F1 score with 'macro' or 'micro' averaging depending on your needs\n",
    "precision = precision_score(y_true_binarized, y_pred_binarized, average='macro')\n",
    "recall = recall_score(y_true_binarized, y_pred_binarized, average='macro')\n",
    "f1 = f1_score(y_true_binarized, y_pred_binarized, average='macro')\n",
    " \n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96aaefa-7011-41df-b16e-1ebbb28c5011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_metric(metric_func, y_true, y_pred, n_bootstrap=1000):\n",
    "    scores = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        indices = np.random.randint(0, len(y_true), len(y_true))\n",
    "        score = metric_func(y_true[indices], y_pred[indices], average='macro', zero_division=0)\n",
    "        scores.append(score)\n",
    "    ci_lower, ci_upper = np.percentile(scores, [2.5, 97.5])\n",
    "    return ci_lower, ci_upper\n",
    " \n",
    "# Calculate 95% confidence intervals for precision, recall, and f1 score\n",
    "precision_ci = bootstrap_metric(precision_score, y_true.values, y_pred.values)\n",
    "recall_ci = bootstrap_metric(recall_score, y_true.values, y_pred.values)\n",
    "f1_ci = bootstrap_metric(f1_score, y_true.values, y_pred.values)\n",
    " \n",
    "print(f'Precision 95% CI: [{precision_ci[0]:.4f}, {precision_ci[1]:.4f}]')\n",
    "print(f'Recall 95% CI: [{recall_ci[0]:.4f}, {recall_ci[1]:.4f}]')\n",
    "print(f'F1 Score 95% CI: [{f1_ci[0]:.4f}, {f1_ci[1]:.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ea5dcc-175d-4cf6-bcff-d3c3d6d6a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Precision', 'Recall', 'F1 Score'],\n",
    "    'Score': [precision, recall, f1],\n",
    "    '95% CI Lower': [precision_ci[0], recall_ci[0], f1_ci[0]],\n",
    "    '95% CI Upper': [precision_ci[1], recall_ci[1], f1_ci[1]]\n",
    "})\n",
    "\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106af173-c733-42eb-96e3-9b0a1221b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv('label_metrics_w_CI_'+model+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f7d5c5-d7bf-4681-a4c0-6de180664b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Precision', 'Recall', 'F1 Score'],\n",
    "    'Score': [precision, recall, f1],\n",
    "    '95% CI Lower': [precision_ci[0], recall_ci[0], f1_ci[0]],\n",
    "    '95% CI Upper': [precision_ci[1], recall_ci[1], f1_ci[1]]\n",
    "})\n",
    "\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea6d5f-af24-4207-95bc-7e6aa2cfb493",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv('label_metrics_w_CI_'+model+'_2lables.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c4aa3-dde4-4555-8951-a18b0c9a3c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

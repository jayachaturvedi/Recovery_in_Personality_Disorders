{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916338ee-983b-4b1d-a703-2dc036ee469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check python version - this script works with 3.11.5, 3.9.7\n",
    "\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed65d29-0cd0-4e54-87f4-df613113d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF USING GPU: check GPU version\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f4664e-ac02-4659-be9d-d37889d8e549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installations if required\n",
    "\n",
    "#!python -m pip install -U setuptools pip\n",
    "#!pip install spacy\n",
    "#!pip install -U 'spacy[cuda12x]' # for GPU\n",
    "#!pip install cupy-cuda12x # for GPU\n",
    "#!pip install cupy # for GPU\n",
    "\n",
    "# Numpy version 1.22.0 works best with this script\n",
    "\n",
    "# !pip install numpy==1.22.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634104a4-7f48-4d17-a44b-e50942c95fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF USING GPU: This should return True\n",
    "\n",
    "print(spacy.prefer_gpu()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca839468-3ac8-4f9b-9ca1-2755d47790d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import other required packages\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    " \n",
    "from spacy.tokens import Doc\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Example\n",
    "from spacy.training import offsets_to_biluo_tags\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.pipeline.spancat import DEFAULT_SPANCAT_MODEL\n",
    "from spacy.pipeline.spancat import DEFAULT_SPANCAT_SINGLELABEL_MODEL\n",
    "from spacy.pipeline.spancat import SpanCategorizer\n",
    "from spacy.pipeline import SpanCategorizer\n",
    "from spacy import displacy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6121dda9-b85d-4e7c-8e2d-99218afbf74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check spacy version - this script works with version 3.7.0, 3.7.4, 3.7.2\n",
    "\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0d56b1-8503-46a0-9b12-61b54fadb537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "model = \"m6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57969f2-88a9-43d6-bc0a-8ce6275d4963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train and test dataset\n",
    "\n",
    "df_train = pd.read_csv('pt_level_train_set_'+model+'.csv')\n",
    "df_test = pd.read_csv('pt_level_test_set_'+model+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9effc1-89c8-4460-b472-31ec8969d26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataframe\n",
    "\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6664e7-36d6-424b-9120-fcbfa8165c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the length of the dataframe\n",
    "\n",
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a970cd85-da6d-44f8-80da-a551e2700957",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f1766-5519-4ae4-bd29-4f7014f72ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column to string type - to avoid errors saying float type does not have x function\n",
    "\n",
    "df_train['TextContent'] = df_train['TextContent'].astype(str)\n",
    "df_train['Text'] = df_train['Text'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c9eeb-edfa-4462-9acf-b9fbe78a2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of each document\n",
    "df_train['document_length'] = df_train['TextContent'].apply(lambda x: len(x)) #TextContent or extracted_sentence\n",
    "\n",
    "# Get minimum, maximum, median, and mean lengths\n",
    "min_length = df_train['document_length'].min()\n",
    "max_length = df_train['document_length'].max()\n",
    "median_length = df_train['document_length'].median()\n",
    "mean_length = df_train['document_length'].mean()\n",
    "\n",
    "# Print the results\n",
    "print(\"Minimum Length:\", min_length)\n",
    "print(\"Maximum Length:\", max_length)\n",
    "print(\"Median Length:\", median_length)\n",
    "print(\"Mean Length:\", mean_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ae9d5-eccb-46b4-987a-95ba7c26979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check label distribution\n",
    "\n",
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e775c24e-ea3e-4f18-b334-f656153d4d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Common text preprocessing\n",
    "text = \" Â  This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs   80/120  .  \"\n",
    " \n",
    "#convert to lowercase and remove punctuations and characters and then strip\n",
    "def preprocess(text):\n",
    "    text = text.lower() #lowercase text\n",
    "    text=text.strip()  #get rid of leading/trailing whitespace\n",
    "    text=re.compile('<.*?>').sub('', text) #Remove HTML tags/markups\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  #Replace punctuation with space\n",
    "    text = re.sub('\\s+', ' ', text)  #Remove extra space and tabs\n",
    "    #text = re.sub(r'\\[[0-9]*\\]',' ',text) #[0-9] matches any digit (0 to 10000...)\n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    #text = re.sub(r'\\d',' ',text) #matches any digit from 0 to 100000..., \\D matches non-digits\n",
    "    text = re.sub(r'\\s+',' ',text) #\\s matches any whitespace, \\s+ matches multiple whitespace, \\S matches non-whitespace\n",
    "    text=re.compile('Â').sub('', text)\n",
    "    text=re.compile('â').sub('', text)\n",
    "    text=text.strip()\n",
    "    \n",
    "    return text\n",
    " \n",
    "text=preprocess(text)\n",
    "print(text)  #text is a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c6504-f2cf-420a-96db-e22d711d2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add or remove columns that require pre-processing\n",
    "\n",
    "df_train['Text'] = df_train['Text'].apply(lambda x: preprocess(x))\n",
    "df_train['TextContent'] = df_train['TextContent'].apply(lambda x: preprocess(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc4bd5b-4443-4d82-bd57-85c706367b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract context around span - this is relevant if using context around span approach - if not, ignore\n",
    "\n",
    "def extract_context_around_span(text, span, context_size=200):#works well with 200\n",
    "    # Find the index of the span in the text\n",
    "    span_index = text.find(span)\n",
    "    \n",
    "    if span_index != -1:\n",
    "        # Calculate the start and end indices for the context around the span\n",
    "        start_index = max(0, span_index - context_size)\n",
    "        end_index = min(len(text), span_index + len(span) + context_size)\n",
    "        \n",
    "        # Extract the context around the span\n",
    "        context_around_span = text[start_index:end_index]\n",
    "        return context_around_span\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173264f0-e80c-4f3f-9f1f-6e199373eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create a new 'ContextAroundSpan' column - ignore if you are not using the context approach\n",
    "\n",
    "df_train['ContextAroundSpan'] = df_train.apply(lambda row: extract_context_around_span(row['TextContent'], row['Text']), axis=1) # Text or NewText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900788d4-be46-4c65-ab7d-9e3142eefac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column to string type - to avoid errors saying float type does not have x function\n",
    "\n",
    "df_train['ContextAroundSpan'] = df_train['ContextAroundSpan'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fc54b2-9091-4fe0-b16b-980236f1f8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataframe\n",
    "\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2259f1b6-c68e-4f78-ab52-06778f5485ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length - ignore if you are not using the context approach\n",
    "df_train['context_length'] = df_train['ContextAroundSpan'].apply(lambda x: len(x))\n",
    "\n",
    "# Get minimum, maximum, median, and mean lengths\n",
    "min_length = df_train['context_length'].min()\n",
    "max_length = df_train['context_length'].max()\n",
    "median_length = df_train['context_length'].median()\n",
    "mean_length = df_train['context_length'].mean()\n",
    "\n",
    "# Print the results\n",
    "print(\"Minimum Length:\", min_length)\n",
    "print(\"Maximum Length:\", max_length)\n",
    "print(\"Median Length:\", median_length)\n",
    "print(\"Mean Length:\", mean_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63568eb-0eb4-4261-bf5c-300ec638ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of span\n",
    "df_train['text_length'] = df_train['Text'].apply(lambda x: len(x))\n",
    "\n",
    "# Get minimum, maximum, median, and mean lengths\n",
    "min_length = df_train['text_length'].min()\n",
    "max_length = df_train['text_length'].max()\n",
    "median_length = df_train['text_length'].median()\n",
    "mean_length = df_train['text_length'].mean()\n",
    "\n",
    "# Print the results\n",
    "print(\"Minimum Length:\", min_length)\n",
    "print(\"Maximum Length:\", max_length)\n",
    "print(\"Median Length:\", median_length)\n",
    "print(\"Mean Length:\", mean_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc8a601-5524-45e7-be7e-8b33a8cf70b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find start and end indices of 'Text' within 'TextContent' or 'NewText' within 'ContextAroundSpan' or 'Text' within 'extracted_sentence'\n",
    "\n",
    "# if you are using the context apprach then use the columns ContextAroundSpan and NewText.\n",
    "# if you are using the extracted sentence approach then use the columns extracted_sentence and Text\n",
    "# else, use TextContent and Text\n",
    "\n",
    "def find_indices(row):\n",
    "    start_index = row['ContextAroundSpan'].find(row['Text'])     # TextContent/ContextAroundSpan/extracted_sentence and Text/NewText/Text\n",
    "    end_index = start_index + len(row['Text'])     # Text or NewText\n",
    "    return start_index, end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd73ed00-096c-43c8-b0fe-a8cde4e5f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some samples\n",
    "\n",
    "df_train['TextContent'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70daabde-fd86-4df8-87c7-8d6f261ff4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Text'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee24cb-6c8e-44a1-baeb-70dfe5a9d77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['ContextAroundSpan'][6] # ignore if you are not using the context approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e3cce-4fc8-467a-948e-53a3f402bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create new columns for start and end indices - make sure you have updatef the find_indices function accordingy for text and span columns\n",
    "\n",
    "df_train[['start_index', 'end_index']] = df_train.apply(find_indices, axis=1, result_type='expand')\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d014f306-eb42-41fa-b8f3-7a7942dc1d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of labels - percentage - 'Domain ' or 'label'\n",
    "\n",
    "df_train['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b48e5a-b968-4bfb-9153-5e89025614ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of labels - numbers\n",
    "\n",
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a53099-bc24-4449-9f48-9e8281fbe930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the find_index function worked\n",
    "\n",
    "start_index = df_train['ContextAroundSpan'][13].find(df_train['Text'][13]) # 'TextContent' or 'ContextAroundSpan' or 'extracted_sentence'\n",
    "start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6230b83d-1629-46fc-bece-ab8c8a2e110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_index = start_index + len(df_train['Text'][13])\n",
    "end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffa3d3-6887-4655-b1be-e422088da66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['start_index'][13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec609b0b-7806-4b8f-b0f7-e1da21620e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base SpaCy model - Here, we load a base SpaCy English model without pre-trained word vectors\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# define your span key name\n",
    "span_key = \"sc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c88ea5-76c3-43b5-b5d4-1c18eb1ef227",
   "metadata": {},
   "source": [
    "TRAINING SET PREPARATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05afcb49-5b6b-46dd-9204-206eade8adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there should be no errors here - convert the dataframe to spacy Example format\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for index, row in df_train.iterrows():# UPDATE - df for context approach or train_df for sentence approach\n",
    "    sentence = row['ContextAroundSpan'] # ContextAroundSpan or TextContent or extracted_sentence\n",
    "    span = row['Text'] # NewText or Text or Text\n",
    "    label = row['label']  # 'Domain ' or 'label'\n",
    "    \n",
    "    \n",
    "    start = row['start_index'] \n",
    "    end = row['end_index'] \n",
    "    \n",
    "    \n",
    "    # Then create a SpaCy Doc object (doc) from the sentence\n",
    "    doc = nlp.make_doc(sentence)\n",
    "\n",
    "    # Ensure the span is found within the sentence and construct an entity tuple if the span is found.\n",
    "    if start != -1 and end <= len(sentence):\n",
    "        entities = [(start, end, label)] # such as entities = [(3389, 3412, \"ADL\")]\n",
    "        tags = spacy.training.offsets_to_biluo_tags(nlp.make_doc(sentence), entities)\n",
    "        # Create a SpaCy Example object and append it to the train_data list, forming the training data for the NER task. - why is label IS_DIGIT\n",
    "        #example = Example.from_dict(doc, {'spans':{span_key: [(start, end, label)]}})  #{\"entities\": entities, \"spans\": {\"sc\": [(start, end, label)]}})\n",
    "        example = Example.from_dict(doc, {\"entities\": entities, \"spans\": {\"sc\": [(start, end, label)]}})\n",
    "        #example = Example.from_dict(doc, {\"entities\": entities})\n",
    "        train_data.append(example)\n",
    "\n",
    "# using this - {'spans':{span_key: [(start, end, label)]}}) - doesn't gove BIO tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c83ea5-c6da-4668-ba83-ee809a742edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the lenght in Example format is same as the length of the dataframe\n",
    "\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b6ffe0-efce-4278-ab32-591f46c2b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the start index againt to make sure it is the same as previous check\n",
    "\n",
    "df_train['start_index'][13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae9e5cf-e0e8-4e38-a66a-253cdd85bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at one of them to make sure entities has the BILOU tags and spans has the values start, end, label\n",
    "\n",
    "train_data[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d613f3-ab10-423e-8626-f9496ac966ef",
   "metadata": {},
   "source": [
    "INITIATE SPANCAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7af006b-9e9b-4681-af22-c088d0ea5355",
   "metadata": {},
   "source": [
    "https://medium.com/data-analytics-at-nesta/a-deep-dive-into-spacys-span-categorisation-model-992024d047c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29be9a-e477-41a2-bfa8-0150eab97f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base SpaCy model - Here, we load a base SpaCy English model without pre-trained word vectors\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# define your span key name - we can also just leave it at entities as will be seen in the prep of train_data?\n",
    "span_key = \"sc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84548e6d-c480-46d1-af7f-86c5367c61ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spancat config \n",
    "config = {\n",
    "    #this refers to the minimum probability to consider a prediction positive\n",
    "    \"threshold\": 0.5,\n",
    "    #the span key refers to the key in doc.spans \n",
    "    \"spans_key\": span_key,\n",
    "    #this refers to the maximum number of labels to consider positive per span\n",
    "    \"max_positive\": None,\n",
    "     #a model instance that is given a list of documents with start end indices representing the labelled spans\n",
    "    \"model\": DEFAULT_SPANCAT_MODEL, # it was DEFAULT_SPANCAT_MODEL, try spacy.SpanCategorizer.v1 didnt work-try DEFAULT_SPANCAT_SINGLELABEL_MODEL-weird preds\n",
    "    #A function that suggests spans. This suggester is fixed n-gram length of up to 3 tokens\n",
    "    #Suggest all spans of at least length min_size and at most length max_size (both inclusive).\n",
    "    \"suggester\": {\"@misc\": \"spacy.ngram_range_suggester.v1\", \"min_size\":1, \"max_size\":30},\n",
    "}\n",
    "#add spancat component to nlp object\n",
    "nlp.add_pipe(\"spancat\", config=config)\n",
    "#get spancat component \n",
    "span=nlp.get_pipe('spancat')\n",
    "\n",
    "#Add labels to spancat component \n",
    "for label in set(df_train['label']): # 'Domain ' or 'label'\n",
    "    span.add_label(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ee5eb9-ca21-4d2b-a689-3c7ffe8b68f0",
   "metadata": {},
   "source": [
    "    '''\n",
    "    This component comes in two forms: spancat and spancat_singlelabel (added in spaCy v3.5.1). \n",
    "    When you need to perform multi-label classification on your spans, use spancat. \n",
    "    The spancat component uses a Logistic layer where the output class probabilities are independent \n",
    "    for each class. However, if you need to predict at most one true class for a span, then use spancat_singlelabel. \n",
    "    It uses a Softmax layer and treats the task as a multi-class problem.\n",
    "\n",
    "    https://spacy.io/api/spancategorizer2\"}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70ec1ff-541e-4abb-bb50-3397251d025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get pipe you want to train on \n",
    "pipe_exceptions = [\"spancat\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# initialise spacy object \n",
    "nlp.initialize()\n",
    "sgd = nlp.create_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6554deb-1ab5-4044-967f-aa9be4b6aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get date and time just before start of training\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "# Display a message indicating what is being printed\n",
    "print(\"Current date and time : \")\n",
    "\n",
    "# Print the current date and time in a specific format\n",
    "print(now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13582903-9fdc-454f-ac24-3972c1a584d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start training the spancat component \n",
    "\n",
    "all_losses = []\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "    for iteration in tqdm(range(150)): #Loops over x (10,20, 50) iterations for training. tqdm is a library used to display progress bars during iterations.\n",
    "        # 20 is probably a good number of iterations - losses dont change much after this.\n",
    "        # shuffling examples before every iteration\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001)) \n",
    "        #Divides the training data into batches. minibatch is a function that generates batches, and compounding is a function used to generate exponentially increasing batch sizes.\n",
    "        for batch in batches:\n",
    "            nlp.update(list(batch), losses=losses, drop=0.1, sgd=sgd)\n",
    "        print(\"epoch: {} Losses: {}\".format(iteration, str(losses)))\n",
    "        all_losses.append(losses['spancat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e287f0a1-90f5-4172-b1ef-381801d9fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the losses as csv to examine for overfit\n",
    "\n",
    "losses = pd.DataFrame(all_losses)\n",
    "losses.to_csv('all_losses_'+model+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa67da32-b90e-4968-9532-30d2a35f55cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "\n",
    "nlp.to_disk(\"trained_spancat_model_\"+model+\"_bypt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886649e1-183f-480d-86cb-c8b2b84ad43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a txt file with the date and time when the training ends\n",
    "\n",
    "from datetime import datetime\n",
    " \n",
    "# get current date and time\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "print(\"Current date & time : \", current_datetime)\n",
    " \n",
    "# convert datetime obj to string\n",
    "str_current_datetime = str(current_datetime)\n",
    " \n",
    "# create a file object along with extension\n",
    "file_name = \"timecompletion\"+str_current_datetime+model+\".txt\"  # UPDATE MODEL NUMBER\n",
    "file = open(file_name, 'w')\n",
    " \n",
    "print(\"File created : \", file.name)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4cfce3-eaa4-4278-a8ea-e66d59d8c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "\n",
    "nlp = spacy.load(\"trained_spancat_model_\"+model+\"_bypt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a1bd88-3d91-4533-9f44-eeaabd3efd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "\n",
    "df_test = pd.read_csv('pt_level_test_set_'+model+'.csv')\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e162ba43-c083-457e-9465-317a564a1793",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50390fab-fc41-4aa1-9c80-6c474207913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract context around span - this is relevant if using context around span approach - if not, ignore\n",
    "\n",
    "def extract_context_around_span(text, span, context_size=200):#works well with 200\n",
    "    # Find the index of the span in the text\n",
    "    span_index = text.find(span)\n",
    "    \n",
    "    if span_index != -1:\n",
    "        # Calculate the start and end indices for the context around the span\n",
    "        start_index = max(0, span_index - context_size)\n",
    "        end_index = min(len(text), span_index + len(span) + context_size)\n",
    "        \n",
    "        # Extract the context around the span\n",
    "        context_around_span = text[start_index:end_index]\n",
    "        return context_around_span\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0e28bd-f639-40a6-a9ed-ff54acde9e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column to string type - to avoid errors saying float type does not have x function\n",
    "\n",
    "df_test['TextContent'] = df_test['TextContent'].astype(str)\n",
    "df_test['Text'] = df_test['Text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47721593-29b5-4ca0-af3c-d54e61cbf964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of each document\n",
    "df_test['document_length'] = df_test['TextContent'].apply(lambda x: len(x))\n",
    "\n",
    "# Get minimum, maximum, median, and mean lengths\n",
    "min_length = df_test['document_length'].min()\n",
    "max_length = df_test['document_length'].max()\n",
    "median_length = df_test['document_length'].median()\n",
    "mean_length = df_test['document_length'].mean()\n",
    "\n",
    "# Print the results\n",
    "print(\"Minimum Length:\", min_length)\n",
    "print(\"Maximum Length:\", max_length)\n",
    "print(\"Median Length:\", median_length)\n",
    "print(\"Mean Length:\", mean_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540be9bb-dca1-462e-97a7-75e1731126df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c718e-7d45-4e10-8945-42cc0c828d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f358651-dd1e-45f3-ba22-1e5c67a7ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Common text preprocessing\n",
    "text = \" Â  This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs   80/120  .  \"\n",
    " \n",
    "#convert to lowercase and remove punctuations and characters and then strip\n",
    "def preprocess(text):\n",
    "    text = text.lower() #lowercase text\n",
    "    text=text.strip()  #get rid of leading/trailing whitespace\n",
    "    text=re.compile('<.*?>').sub('', text) #Remove HTML tags/markups\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  #Replace punctuation with space\n",
    "    text = re.sub('\\s+', ' ', text)  #Remove extra space and tabs\n",
    "    #text = re.sub(r'\\[[0-9]*\\]',' ',text) #[0-9] matches any digit (0 to 10000...)\n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    #text = re.sub(r'\\d',' ',text) #matches any digit from 0 to 100000..., \\D matches non-digits\n",
    "    text = re.sub(r'\\s+',' ',text) #\\s matches any whitespace, \\s+ matches multiple whitespace, \\S matches non-whitespace\n",
    "    text = re.compile('Â').sub('', text)  \n",
    "    text = re.compile('â').sub('', text) \n",
    "    text=text.strip()  #get rid of leading/trailing whitespace\n",
    "    \n",
    "    return text\n",
    " \n",
    "text=preprocess(text)\n",
    "print(text)  #text is a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16de438-9995-4773-9b59-3d8154d1168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Text'] = df_test['Text'].apply(lambda x: preprocess(x))\n",
    "df_test['TextContent'] = df_test['TextContent'].apply(lambda x: preprocess(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade7495d-1912-4b1b-b57b-21336c63d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip if not doing context approach - or if v2 of test set file\n",
    "\n",
    "df_test['ContextAroundSpan'] = df_test.apply(lambda row: extract_context_around_span(row['TextContent'], row['Text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05f0a6-bcea-40eb-998d-b6ffbf0c52c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['ContextAroundSpan'] = df_test['ContextAroundSpan'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d045fe-4b95-46df-9e66-48b5c009d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86228a33-1c6b-4cdb-9b97-c64663a1c468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test.to_csv('test_error.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e9f946-26c0-48bc-867d-d73e5c3ed96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test['ContextAroundSpan'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665fd274-057d-4ef3-9891-71ce86e99bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test = df_test.dropna(subset=['ContextAroundSpan'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0f8e02-d7da-467a-bc81-ad37f63ed1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['start_index', 'end_index']] = df_test.apply(find_indices, axis=1, result_type='expand')\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0858e61d-78b7-49f0-9172-75ede8aced72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the find_index function worked - all indexes don't work when test_df is split from spacy sentences file at the beginning\n",
    "\n",
    "start_index = df_test['TextContent'][13].find(df_test['Text'][13])\n",
    "#start_index = df['ContextAroundSpan'][1].find(df['NewText'][1])\n",
    "start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ef5d3-fea4-4d32-b428-971abf219db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_index = start_index + len(df_test['Text'][13])\n",
    "#end_index = start_index + len(df['NewText'][1])\n",
    "end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ca9835-4e40-4593-9c21-b3c48f4125c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spancat_predictions(document):\n",
    "    doc = nlp(document)\n",
    "    spancat_predictions = [(span.text, span.label_) for span in doc.spans['sc']]\n",
    "    return spancat_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a6dbba-8205-43c6-a4b8-c1da66a84e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'TextContent' or 'ContextAroundSpan' or 'extracted_sentence' column of the DataFrame and store the predictions in a new column\n",
    "df_test['SpanCat_Predictions'] = df_test['ContextAroundSpan'].apply(get_spancat_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a58c885-5f33-412e-9da0-4e6a29619762",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61980f64-92b7-4a4d-986c-5f8141edf09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('test_data_w_'+model+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d0bd4-7472-4309-a810-6afe36149fc0",
   "metadata": {},
   "source": [
    "Or quick test on few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d92f1e-3cab-40b8-9a0d-1137fec3c5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New sentences to test the model - tried both TextContent (documents) and NewText(spans)\n",
    "new_sentences = list(df_test['ContextAroundSpan'][0:5])\n",
    " \n",
    "# Process each new sentence with the trained model\n",
    "for sentence in new_sentences:\n",
    "    doc = nlp(sentence)\n",
    " \n",
    "    # Access SpanCat predictions (assuming they are in the 'spancat' or 'sc' attribute of Doc)\n",
    "    #spancat_predictions = [span.text for span in doc.spans['sc']]\n",
    "    spancat_predictions = [(span.text, span.label_) for span in doc.spans['sc']]\n",
    " \n",
    "    print(f\"Sentence: '{sentence}'\")\n",
    "    print(\"SpanCat Predictions:\", spancat_predictions)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dcfc04-21aa-49bf-8ef6-d7ec363efb84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a023e8b-0e39-4cdf-8e4a-6afcf80cc357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ed58e-c49b-461e-959a-b9993fda3a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
